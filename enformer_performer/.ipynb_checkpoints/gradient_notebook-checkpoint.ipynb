{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f89352f1-c3bf-4225-9de9-d4e1117998bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 12:41:06.596227: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-15 12:41:06.817307: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-15 12:41:06.817347: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-15 12:41:07.914713: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-15 12:41:07.914883: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-15 12:41:07.914900: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import re\n",
    "import argparse\n",
    "import collections\n",
    "import gzip\n",
    "import math\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import logging\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "#silence_tensorflow()\n",
    "os.environ['TPU_LOAD_LIBRARY']='0'\n",
    "os.environ['TF_ENABLE_EAGER_CLIENT_STREAMING_ENQUEUE']='False'\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import strings as tfs\n",
    "from tensorflow.keras import mixed_precision\n",
    "from scipy.stats.stats import pearsonr  \n",
    "from scipy.stats.stats import spearmanr  \n",
    "## custom modules\n",
    "import enformer_performer as enformer_performer\n",
    "#import src.aformer_TF as aformer\n",
    "\n",
    "\n",
    "import training_utils as training_utils\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "import kipoiseq\n",
    "from kipoiseq import Interval\n",
    "import pyfaidx\n",
    "from deeplift import dinuc_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9453310b-f3ab-4159-8a92-aa0982fd2f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 12:41:09.648372: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-15 12:41:09.648436: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-15 12:41:09.648464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tpu-genformer-v2-6): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 12:41:09.969572: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-15 12:41:09.997739: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:447] Started server with target: grpc://localhost:39958\n",
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: node-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: node-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='node-5')\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.TPUStrategy(resolver)\n",
    "\n",
    "with strategy.scope():\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.FILE\n",
    "    options.deterministic=False\n",
    "    #options.experimental_threading.max_intra_op_parallelism = 1\n",
    "    mixed_precision.set_global_policy('mixed_bfloat16')\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    #options.num_devices = 64\n",
    "\n",
    "    BATCH_SIZE_PER_REPLICA = 1\n",
    "    NUM_REPLICAS = strategy.num_replicas_in_sync\n",
    "    GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * NUM_REPLICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74340db5-b155-4ffc-93af-f6bc136a6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    heads_dict = {}\n",
    "    orgs = [\"hg\"]\n",
    "    for k, org in enumerate(orgs):\n",
    "        heads_dict[org] = int(k)\n",
    "        \n",
    "        \n",
    "    class Enformer:\n",
    "        def __init__(self):\n",
    "\n",
    "            #inits=training_utils.get_initializers(\"gs://picard-testing-176520/enformer_performer/models/enformer_performer_230214_E-P-_2696_enformer_393k_load_init-True_freeze-True_LR1-2.5e-06_LR2-8e-05_T-4_F-1536_K-relu_kernel_transformation/iteration_5\")\n",
    "            model = enformer_performer.enformer_performer( num_transformer_layers = 6,\n",
    "                         num_heads= 8,\n",
    "                         heads_channels = {'human': 2696,\n",
    "                                                 'mouse': 987},\n",
    "                         filter_list=[768,896,1024,1152,1280,1536],\n",
    "                         dim=192,\n",
    "                         d_model=1536,\n",
    "                         norm=True,\n",
    "                         max_seq_length=3072,\n",
    "                         nb_random_features=256,\n",
    "                         hidden_size=1536,\n",
    "                         numerical_stabilizer=0.001,\n",
    "                         attention_dropout_rate=0.05,\n",
    "                         dropout_rate=0.40,\n",
    "                         BN_momentum=0.90,\n",
    "                         rel_pos_bins=3072,\n",
    "                         out_length = 3072,\n",
    "                         target_length = 896,\n",
    "                         use_mask_pos=False,\n",
    "                         use_rot_emb=True,\n",
    "                         load_init=False,\n",
    "                         freeze_conv_layers=False,\n",
    "                         stable_variant=True,\n",
    "                         inits=None,\n",
    "                         kernel_transformation=\"relu_kernel_transformation\",\n",
    "                         normalize=True,\n",
    "                         seed=5,\n",
    "                         name='enformer_performer',\n",
    "                         use_max_pool=False,\n",
    "                         block_type='enformer')\n",
    "            seq = tf.ones((1,393216,4))\n",
    "\n",
    "\n",
    "            pred = model(seq,training=False)\n",
    "            print('built model')\n",
    "            model.load_weights(\"gs://picard-testing-176520/enformer_performer/models/enformer_performer_230214_E-P-_2696_enformer_393k_load_init-True_freeze-True_LR1-2.5e-06_LR2-8e-05_T-4_F-1536_K-relu_kernel_transformation/iteration_15/saved_model\")\n",
    "            self._model=model\n",
    "\n",
    "        def predict_on_batch(self, inputs):\n",
    "            return self._model.predict_on_batch(inputs)[0]\n",
    "\n",
    "        @tf.function\n",
    "        def contribution_input_grad(self, input_sequence,\n",
    "                                    target_mask, track_index):\n",
    "            input_sequence = input_sequence[tf.newaxis]\n",
    "\n",
    "            target_mask_mass = tf.reduce_sum(target_mask)\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(input_sequence)\n",
    "                pred = self._model.predict_on_batch(input_sequence)[0]['human'][:,:,track_index]\n",
    "\n",
    "                print(target_mask[tf.newaxis].shape)\n",
    "                print(pred.shape)\n",
    "                prediction = tf.reduce_sum(\n",
    "                      target_mask[tf.newaxis] * pred) / target_mask_mass\n",
    "\n",
    "            grad = tape.gradient(prediction, input_sequence)\n",
    "            input_grad = grad * input_sequence\n",
    "            input_grad = tf.squeeze(input_grad, axis=0)\n",
    "\n",
    "            return tf.reduce_sum(input_grad, axis=-1), grad\n",
    "        \n",
    "        \n",
    "    # @title `variant_centered_sequences`\n",
    "    #with strategy.scope():\n",
    "    class FastaStringExtractor:\n",
    "\n",
    "        def __init__(self, fasta_file):\n",
    "            self.fasta = pyfaidx.Fasta(fasta_file)\n",
    "            self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n",
    "\n",
    "        def extract(self, interval: Interval, **kwargs) -> str:\n",
    "            # Truncate interval if it extends beyond the chromosome lengths.\n",
    "            chromosome_length = self._chromosome_sizes[interval.chrom]\n",
    "            trimmed_interval = Interval(interval.chrom,\n",
    "                                        max(interval.start, 0),\n",
    "                                        min(interval.end, chromosome_length),\n",
    "                                        )\n",
    "            # pyfaidx wants a 1-based interval\n",
    "            sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n",
    "                                              trimmed_interval.start + 1,\n",
    "                                              trimmed_interval.stop).seq).upper()\n",
    "            # Fill truncated values with N's.\n",
    "            pad_upstream = 'N' * max(-interval.start, 0)\n",
    "            pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n",
    "            return pad_upstream + sequence + pad_downstream\n",
    "\n",
    "        def close(self):\n",
    "            return self.fasta.close()\n",
    "        \n",
    "        \n",
    "    def one_hot_encode(sequence):\n",
    "        return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n",
    "\n",
    "\n",
    "    def importance_scores(chrom, start, stop, target_index, mask_indices):\n",
    "\n",
    "        target_interval = kipoiseq.Interval(chrom, int(start), int(stop))\n",
    "        resized_interval = target_interval.resize(393216)\n",
    "        sequence_one_hot = one_hot_encode(fasta_extractor.extract(resized_interval))\n",
    "        #print(sequence_one_hot.shape)\n",
    "        #print(sequence_one_hot[np.newaxis].shape)\n",
    "        predictions = model.predict_on_batch(sequence_one_hot[np.newaxis])['human'][0,:,:]\n",
    "\n",
    "        target_mask = np.zeros_like(predictions)\n",
    "        for idx in mask_indices:\n",
    "            target_mask[idx, target_index] = 1\n",
    "        print(target_mask.shape)\n",
    "        # This will take some time since tf.function needs to get compiled.\n",
    "        contribution_scores, grad = model.contribution_input_grad(sequence_one_hot.astype(np.float32), target_mask, target_index)\n",
    "        contribution_scores = contribution_scores.numpy()\n",
    "        pooled_contribution_scores = tf.nn.avg_pool1d(np.abs(contribution_scores)[np.newaxis,\n",
    "                                                                                  :, np.newaxis],\n",
    "                                                      128, 128, 'VALID')[0, :, 0].numpy()\n",
    "\n",
    "        base_scores = (sequence_one_hot[:][:].T * [contribution_scores[:],\n",
    "                                                       contribution_scores[:],\n",
    "                                                       contribution_scores[:],\n",
    "                                                       contribution_scores[:]]).T\n",
    "\n",
    "        gradient = np.multiply(sequence_one_hot[:][:].T, (np.squeeze(grad).T))\n",
    "        ###### dinucleotide shuffled sequences\n",
    "        seq_shuffled = dinuc_shuffle.dinuc_shuffle(sequence_one_hot, 1)[0]\n",
    "\n",
    "        target_mask = np.zeros_like(predictions)\n",
    "        for idx in mask_indices:\n",
    "            target_mask[idx, target_index] = 1\n",
    "        # This will take some time since tf.function needs to get compiled.\n",
    "\n",
    "        contribution_scores_scram, grad_scram = model.contribution_input_grad(seq_shuffled, target_mask, target_index)\n",
    "        contribution_scores_scram = contribution_scores_scram.numpy()\n",
    "        pooled_contribution_scores_scram = tf.nn.avg_pool1d(np.abs(contribution_scores_scram)[np.newaxis,\n",
    "                                                                                  :, np.newaxis],\n",
    "                                                      128, 128, 'VALID')[0, :, 0].numpy()\n",
    "\n",
    "        ## get base level matrix\n",
    "\n",
    "        base_scores_scram = (seq_shuffled[:][:].T * [contribution_scores_scram[:],\n",
    "                                            contribution_scores_scram[:],\n",
    "                                            contribution_scores_scram[:],\n",
    "                                            contribution_scores_scram[:]]).T\n",
    "\n",
    "        ## get base level matri\n",
    "        gradient_scram = np.multiply(seq_shuffled[:][:].T, (np.squeeze(grad_scram).T))\n",
    "\n",
    "\n",
    "\n",
    "        return resized_interval,contribution_scores,pooled_contribution_scores,base_scores,np.squeeze(grad), sequence_one_hot,base_scores_scram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58aadf33-0506-4bf6-a752-5097a14a6920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out_bedgraph_pooled(pooled_contribution_scores, interval, filename_base):\n",
    "    start = interval.start\n",
    "    end =  interval.end\n",
    "    chrom = interval.chrom\n",
    "    name = '_'.join([str(chrom), str(start), str(end)])\n",
    "\n",
    "    out_file = open(filename_base + '.pooled.bedGraph', 'w')\n",
    "\n",
    "\n",
    "    for k, value in enumerate(pooled_contribution_scores):\n",
    "\n",
    "        start_interval = k * 128 + start\n",
    "        end_interval = (k+1) * 128 + start\n",
    "\n",
    "        line = [str(chrom),\n",
    "                str(start_interval), str(end_interval),\n",
    "                str(value)]\n",
    "\n",
    "        out_file.write('\\t'.join(line) + '\\n')\n",
    "    out_file.close()\n",
    "    \n",
    "    \n",
    "def write_out_bedgraph_all(contribution_scores, interval, filename_base):\n",
    "    start = interval.start\n",
    "    end =  interval.end\n",
    "    chrom = interval.chrom\n",
    "    name = '_'.join([str(chrom), str(start), str(end)])\n",
    "\n",
    "    out_file = open(filename_base + '.all.bedGraph', 'w')\n",
    "\n",
    "\n",
    "    for k, value in enumerate(contribution_scores):\n",
    "\n",
    "        start_interval = start + k\n",
    "        end_interval = start + k + 1\n",
    "\n",
    "        line = [str(chrom),\n",
    "                str(start_interval), str(end_interval),\n",
    "                str(value)]\n",
    "\n",
    "        out_file.write('\\t'.join(line) + '\\n')\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cb95586-2026-4644-b21a-323105ff2d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Faidx(\"/home/jupyter/reference/hg38.fa\")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## iterator\n",
    "#with strategy.scope():\n",
    "model = Enformer()\n",
    "fasta_file = \"/home/jupyter/reference/hg38.fa\"\n",
    "pyfaidx.Faidx(fasta_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2c5b19e-85a2-4d9f-80fc-d731a6634a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 896, 2696)\n",
      "(1, 896)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_7190/1537788309.py\", line 65, in contribution_input_grad  *\n        prediction = tf.reduce_sum(\n\n    ValueError: Dimensions must be equal, but are 2696 and 896 for '{{node mul}} = Mul[T=DT_BFLOAT16](strided_slice_3, strided_slice_1)' with input shapes: [1,896,2696], [1,896].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7190/2381380792.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmask_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m445\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m446\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m447\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m448\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m449\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportance_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chr8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m127736151\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m127736909\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2494\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Jurkat resting corresponds to index 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mresized_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_one_hot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbase_scores_shuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m write_out_bedgraph_pooled(pooled,\n",
      "\u001b[0;32m/tmp/ipykernel_7190/1537788309.py\u001b[0m in \u001b[0;36mimportance_scores\u001b[0;34m(chrom, start, stop, target_index, mask_indices)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mtarget_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# This will take some time since tf.function needs to get compiled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mcontribution_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontribution_input_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_one_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mcontribution_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontribution_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         pooled_contribution_scores = tf.nn.avg_pool1d(np.abs(contribution_scores)[np.newaxis,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/__autograph_generated_filels49d_7e.py\u001b[0m in \u001b[0;36mtf__contribution_input_grad\u001b[0;34m(self, input_sequence, target_mask, track_index)\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_mask_mass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0minput_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_7190/1537788309.py\", line 65, in contribution_input_grad  *\n        prediction = tf.reduce_sum(\n\n    ValueError: Dimensions must be equal, but are 2696 and 896 for '{{node mul}} = Mul[T=DT_BFLOAT16](strided_slice_3, strided_slice_1)' with input shapes: [1,896,2696], [1,896].\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## center interval at the MYC promoter\n",
    "chrom = \"chr8\"\n",
    "start = 127736151\n",
    "end = 127736909\n",
    "#SEQUENCE_LENGTH=196608*2 # add 6 bp to allow for the +/- 0-3 bp shift\n",
    "#target_length = 196608\n",
    "fasta_extractor = FastaStringExtractor(fasta_file)\n",
    "\n",
    "mask_indices=[445,446,447,448,449]\n",
    "out = importance_scores(\"chr8\", 127736151, 127736909,2494, mask_indices) # Jurkat resting corresponds to index 3\n",
    "resized_int, scores, pooled, base_scores,grad, seq_one_hot,base_scores_shuff = out\n",
    "write_out_bedgraph_pooled(pooled,\n",
    "                          resized_int,\n",
    "                          'k562_393k_test.pool.bedGraph')\n",
    "write_out_bedgraph_all(np.abs(scores),\n",
    "                          resized_int,\n",
    "                          'k562_393k_test.bedGraph')\n",
    "\n",
    "#np.save(\"Jurkat_resting.basescores.npy\", np.array(base_scores))#, fmt='%10.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cfe8da-ad47-4d1a-821a-c6369ae808f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-cpu.2-6.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-6:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
